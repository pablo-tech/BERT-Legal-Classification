{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CaseText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in treatments_list.json is a list of dicts. \n",
    "1. The full text of the case is in the \"text\" key of each dict\n",
    "2. The date the case was decided (in unix time) is in the \"decide_date\" key of each dict\n",
    "3. A code for the court that decided the case is in the \"jurisdiction_code\" key of each dict\n",
    "4. The title of the case is in the \"title\" key of each dict\n",
    "5. A list of flag codes is in the \"treatments\" key of each dict\n",
    "\n",
    "The data in dissenting_opinions.csv is a csv with 3 columns that mean the following:\n",
    "1. \"id\" an ID for that entry, probably not useful to you\n",
    "2. \"label\" - 1 if this text comes from the end of the majority opinion of a case in which there was a dissent (more on this in a minute), or 0 if it comes from any other part of the case. \n",
    "3. \"text\" - the text of a part of a case\n",
    "\n",
    "Your task will be to correctly predict when a case should have an 'r', 'v', or 'a' flag. These stand for \"reversing\", \"vacating\" or \"affirming\". One case can sometimes affirm in part and vacate or reverse in part. In these situations, we want the case to be given both flags that apply to it. The text where the affirming/reversing/vacating occurs is usually (but not always) at the end of the case. Where a case has a \"dissenting\" or \"concurring\" opinion, the relevant language will usually appear at the end of the \"majority\" opinion, right before the \"dissenting\" or \"concurring\" opinion begins.  Here is an example of text where an affirming occurs:\n",
    "\n",
    "\"Widomski failed to demonstrate that OCCC's explanation for its decision to bring disciplinary proceedings against him was pretext for retaliation. “Claims for retaliation [under the ADA] are analyzed under the same burden-shifting framework established for Title VII cases.” Treglia v. Town of Manlius, 313 F.3d 713, 719 (2d Cir.2002). For substantially the reasons set forth in the District Court's Opinion and Order entered March 21, 2013, we agree that Widomski has not produced evidence that would permit a reasonable factfinder to conclude that the initiation of disciplinary proceedings against him was a pretext for retaliation. Contarino's good faith belief that Widomski fabricated two assignments constitutes a legitimate, nonretaliatory reason for bringing a disciplinary action against him. The burden having shifted back to Widomski to provide competent evidence of pretext, Widomski fails to raise a genuine factual dispute as to whether this explanation is false or otherwise pretextual. See Weinstock v. Columbia Univ., 224 F.3d 33, 42 (2d Cir.2000). CONCLUSION We have considered Widomski's remaining arguments and conclude that they are without merit. For the foregoing reasons, the judgment of the District Court is AFFIRMED.\"\n",
    "\n",
    "As I said earlier today, you probably want to use a pre-trained transformer model, though feel free to use another approach if you think it would be better. I'd suggest getting a working classifier set up using just the data from treatments_list.json before attempting to use the data in dissenting_opinions.csv to try to help further. You should be able to do reasonably well with just the data in treatments_list.json. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JigSaw: Toxic Online Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CaseText notebook was leveraged from prior NLP work on JigSaw's Kaggle competititon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "# Notebook: https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorial-04-text-classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCIES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ktrain in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.17.2)\n",
      "Requirement already satisfied: ipython in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (7.13.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.23.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (1.4.1)\n",
      "Requirement already satisfied: keras-bert>=0.81.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (0.84.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (3.1.0)\n",
      "Requirement already satisfied: cchardet==2.1.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.1.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (3.2.1)\n",
      "Requirement already satisfied: seqeval in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (0.0.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (20.3)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (1.0.3)\n",
      "Requirement already satisfied: whoosh in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.7.4)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (0.14.1)\n",
      "Requirement already satisfied: transformers>=2.11.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.11.0)\n",
      "Requirement already satisfied: jieba in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (0.42.1)\n",
      "Requirement already satisfied: networkx>=2.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.4)\n",
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (1.0.8)\n",
      "Requirement already satisfied: scikit-learn==0.21.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (0.21.3)\n",
      "Requirement already satisfied: syntok in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (1.3.1)\n",
      "Requirement already satisfied: tensorflow==2.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.1.0)\n",
      "Requirement already satisfied: bokeh in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (2.0.1)\n",
      "Requirement already satisfied: fastprogress>=0.1.21 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ktrain) (0.2.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (3.0.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (46.1.3.post20200330)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (2.6.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (4.8.0)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (0.1.0)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (0.15.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (4.3.3)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from ipython->ktrain) (0.7.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->ktrain) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->ktrain) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->ktrain) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->ktrain) (2020.4.5.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from scipy==1.4.1->ktrain) (1.18.1)\n",
      "Requirement already satisfied: Keras in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-bert>=0.81.0->ktrain) (2.2.4)\n",
      "Requirement already satisfied: keras-transformer>=0.37.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-bert>=0.81.0->ktrain) (0.37.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (4.44.1)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (0.3.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (3.12.2)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (0.9.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (0.18.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (19.3.0)\n",
      "Requirement already satisfied: promise in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (2.3)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (1.1.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (0.22.2)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-datasets->ktrain) (1.12.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib>=3.0.0->ktrain) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pandas>=1.0.1->ktrain) (2019.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers>=2.11.0->ktrain) (2020.6.8)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers>=2.11.0->ktrain) (0.7)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers>=2.11.0->ktrain) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers>=2.11.0->ktrain) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers>=2.11.0->ktrain) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from transformers>=2.11.0->ktrain) (0.1.91)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (0.34.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (1.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (3.2.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (2.1.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (1.27.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow==2.1.0->ktrain) (0.2.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bokeh->ktrain) (5.3.1)\n",
      "Requirement already satisfied: pillow>=4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bokeh->ktrain) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bokeh->ktrain) (3.7.4.1)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bokeh->ktrain) (2.11.1)\n",
      "Requirement already satisfied: tornado>=5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bokeh->ktrain) (6.0.4)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ktrain) (0.1.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
      "Requirement already satisfied: parso>=0.5.2 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from jedi>=0.10->ipython->ktrain) (0.5.2)\n",
      "Requirement already satisfied: ipython-genutils in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from Keras->keras-bert>=0.81.0->ktrain) (2.10.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.7.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.14.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.6.0)\n",
      "Requirement already satisfied: keras-multi-head>=0.27.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.27.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.11.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-metadata->tensorflow-datasets->ktrain) (1.52.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from sacremoses->transformers>=2.11.0->ktrain) (7.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.18.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
      "Requirement already satisfied: keras-self-attention==0.46.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.37.0->keras-bert>=0.81.0->ktrain) (0.46.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.4.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (4.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (2.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->ktrain) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ktrain is a lightweight wrapper for the deep learning library Keras\n",
    "# https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c\n",
    "# https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c\n",
    "# https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorial-04-text-classification.ipynb\n",
    "\n",
    "!pip install ktrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ktrain\n",
    "\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktrain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model to use\n",
    "learner_name =  'bert' \n",
    "# learner_name =  'fasttext' \n",
    "\n",
    "## dataset to ue\n",
    "data_set = 'jigsaw'\n",
    "# data_set = 'casetext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path to data\n",
    "NLP_ROOT = '/home/ec2-user/SageMaker/efs/NLP/'\n",
    "JIGSAW_FOLDER = 'JigSaw/'\n",
    "CASE_FOLDER = 'CaseText/'\n",
    "JIGSAW_ROOT = './' # NLP_ROOT + JIGSAW_FOLDER\n",
    "CASETEXT_ROOT = NLP_ROOT + CASE_FOLDER\n",
    "\n",
    "if data_set == 'jigsaw':\n",
    "    NLP_ROOT = JIGSAW_ROOT\n",
    "if data_set == 'casetext':\n",
    "    NLP_ROOT = CASETEXT_ROOT\n",
    "    \n",
    "## max num of words to consider in vocabulary\n",
    "NUM_WORDS = 50000\n",
    "\n",
    "## each document can be of most <maxlen> words. 0 is used as padding ID.\n",
    "MAX_LEN = 150\n",
    "\n",
    "## batch_size based on recommendations from Google, depends on sequence_length\n",
    "# https://github.com/google-research/bert/blob/master/README.md\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "## proportion of training to use for validation\n",
    "# if None, 10% of data will be used for validation\n",
    "VAL_PCT = None \n",
    "\n",
    "## size of multi-word phrases to consider: 2 will consider both 1-word phrases and 2-word phrases     \n",
    "NGRAM_RANGE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JigSaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train.csv ./test.csv\n"
     ]
    }
   ],
   "source": [
    "if data_set == 'jigsaw':\n",
    "    x_col_name = 'comment_text'\n",
    "    y_labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    TRAIN_PATH = JIGSAW_ROOT + 'train.csv'\n",
    "    TEST_PATH = JIGSAW_ROOT + 'test.csv'\n",
    "    print(TRAIN_PATH, TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CaseText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_set == 'casetext':\n",
    "    TREATMENT_PATH = CASETEXT_ROOT + 'treatments_list.json'\n",
    "    DISSENTING_PATH = CASETEXT_ROOT + 'dissenting_opinions.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JigSaw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train= (159571, 8)\n",
      "test= (153164, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(\"train=\", train.shape)\n",
    "print(\"test=\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_text = train[x_col_name][0]\n",
    "a_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Longer sequences are disproportionately expensive because attention is quadratic to the sequence length\n",
    "# Publication https://github.com/google-research/bert/blob/master/README.md\n",
    "# An approach https://github.com/google-research/bert/issues/27\n",
    "len(a_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CaseText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_set == 'casetext':\n",
    "    with open(TREATMENT_PATH) as json_file:\n",
    "        treatments = json.load(json_file)\n",
    "        \n",
    "    count = len(treatments)    \n",
    "    dic =  treatments[2]   \n",
    "    case_text = dic['text'] \n",
    "    date = dic['decide_date']\n",
    "    court = dic['jurisdiction_code']\n",
    "    title = dic['title']\n",
    "    treatment = dic['treatments']\n",
    "    print(\"count=\", count, \"treatment=\", treatment, \"court=\", court, \"date=\", date, \"len=\", len(case_text), \"title=\", title, \"text=\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dissenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_set == 'casetext':\n",
    "    dissenting = pd.read_csv(DISSENTING_PATH)\n",
    "    # \"label\": \n",
    "    # - 1 if this text comes from the end of the majority opinion of a case in which there was a dissent \n",
    "    # - 0 if it comes from any other part of the case.\n",
    "    # \"text\": part of the case\n",
    "    dissenting.head()\n",
    "    dissenting['text'][0]\n",
    "    dissenting['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_PATH= ./train.csv \n",
      "VAL_PCT= None \n",
      "x_col_name= comment_text \n",
      "y_labels= ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "NUM_WORDS= 50000 \n",
      "MAX_LEN= 150 \n",
      "NGRAM_RANGE= 1 \n",
      "preprocess_mode= bert\n"
     ]
    }
   ],
   "source": [
    "# BERT expects input data in a specific format, \n",
    "# with special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). \n",
    "# Furthermore, we need to tokenize our text into tokens that correspond to BERT’s vocabulary.\n",
    "\"\"\"\n",
    "x_col_name: column containing the text\n",
    "y_labels: list of columns that are to be treated as labels\n",
    "learner_name: 'bert', 'fasttext'\n",
    "TRAIN_PATH: path to training set\n",
    "TEST_PATH: path to test set\n",
    "NUM_WORDS: max num of words to consider in vocabulary\n",
    "MAX_LEN: each document can be of most <maxlen> words. 0 is used as padding ID.\n",
    "Source: https://github.com/amaiya/ktrain/blob/7213e3ac35b71e7cad8c188a494d2b310a871d61/ktrain/text/data.py#L121\n",
    "\"\"\"\n",
    "\n",
    "### Additional texts_from_csv options:\n",
    "# preprocess_mode='standard' (normal tokenization) or 'bert' tokenization and preprocessing\n",
    "# encoding=None,  # auto-detected\n",
    "# lang=None,      # auto-detected\n",
    "# sep=',', random_state=None,       \n",
    "# verbose=1\n",
    "\n",
    "if learner_name == 'bert':\n",
    "    preprocess_mode = learner_name\n",
    "else:\n",
    "    preprocess_mode = 'standard'\n",
    "\n",
    "# print config    \n",
    "print(\"TRAIN_PATH=\", TRAIN_PATH, \"\\nVAL_PCT=\", VAL_PCT, \"\\nx_col_name=\", x_col_name, \"\\ny_labels=\", y_labels)\n",
    "print(\"NUM_WORDS=\", NUM_WORDS, \"\\nMAX_LEN=\", MAX_LEN, \"\\nNGRAM_RANGE=\", NGRAM_RANGE, \"\\npreprocess_mode=\", preprocess_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detected encoding: utf-8 (if wrong, set manually)\n",
      "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
      "[██████████████████████████████████████████████████]\n",
      "extracting pretrained BERT model...\n",
      "done.\n",
      "\n",
      "cleanup downloaded zip...\n",
      "done.\n",
      "\n",
      "preprocessing train...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "preprocessing test...\n",
      "language: en\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "done."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# return preprocessed the texts and the pre-processor    \n",
    "(x_train, y_train), (x_test, y_test), pre_processor = text.texts_from_csv(train_filepath=TRAIN_PATH,\n",
    "                                                                         val_pct=VAL_PCT, \n",
    "                                                                         text_column=x_col_name,\n",
    "                                                                         label_columns = y_labels,\n",
    "                                                                         max_features=NUM_WORDS, \n",
    "                                                                         maxlen=MAX_LEN,\n",
    "                                                                         ngram_range=NGRAM_RANGE,\n",
    "                                                                         preprocess_mode=preprocess_mode\n",
    "                                                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Transformed Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 1000, 1043, ...,    0,    0,    0],\n",
       "       [ 101, 2017, 2074, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2106, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 2017, 2113, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2031, ...,    0,    0,    0],\n",
       "       [ 101, 1045, 2253, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(y_labels)\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pre_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext: a fastText-like model [http://arxiv.org/pdf/1607.01759.pdf]\n",
      "logreg: logistic regression using a trainable Embedding layer\n",
      "nbsvm: NBSVM model [http://www.aclweb.org/anthology/P12-2018]\n",
      "bigru: Bidirectional GRU with pretrained fasttext word vectors [https://fasttext.cc/docs/en/crawl-vectors.html]\n",
      "standard_gru: simple 2-layer GRU with randomly initialized embeddings\n",
      "bert: Bidirectional Encoder Representations from Transformers (BERT) [https://arxiv.org/abs/1810.04805]\n",
      "distilbert: distilled, smaller, and faster BERT from Hugging Face [https://arxiv.org/abs/1910.01108]\n"
     ]
    }
   ],
   "source": [
    "# available classifiers\n",
    "text.print_text_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Multi-Label? True\n",
      "maxlen is 150\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"    \n",
    "Instantiate a classifier, and wrap it in a learner with data\n",
    "\"\"\"\n",
    "\n",
    "# load a pretrained model with a randomly initialized final Dense layer\n",
    "# freeze_layers=80\n",
    "# Source: https://github.com/amaiya/ktrain/blob/7213e3ac35b71e7cad8c188a494d2b310a871d61/ktrain/text/learner.py#L14\n",
    "model = text.text_classifier(name=learner_name, \n",
    "                             train_data=(x_train, y_train), \n",
    "                             preproc=pre_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "Wrap a classifier in a learner with data\n",
    "\"\"\"\n",
    "\n",
    "# wrap model and data in ktrain.Learner object\n",
    "# workers=1, use_multiprocessing=False, multigpu=False\n",
    "learner = ktrain.get_learner(model=model, \n",
    "                             train_data=(x_train, y_train), \n",
    "                             val_data=(x_test, y_test),\n",
    "                             batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c\n",
    "# Notebook: https://github.com/amaiya/ktrain/blob/master/tutorial-02-tuning-learning-rates.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulating training for different learning rates... this may take a few moments...\n",
      "Train on 119678 samples\n",
      "Epoch 1/1024\n",
      "  7146/119678 [>.............................] - ETA: 1:11:15 - loss: 0.2669 - accuracy: 0.8910\n",
      "\n",
      "done.\n",
      "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
     ]
    }
   ],
   "source": [
    "# briefly simulate training\n",
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visually identify best learning rate\n",
    "learner.lr_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will use:  learning_rate= 0.0001 num_epoch= 1\n"
     ]
    }
   ],
   "source": [
    "# Thus use learning rate \n",
    "learning_rate = 1 * pow(10, -4)\n",
    "num_epoch = 1\n",
    "print(\"will use: \", \"learning_rate=\", learning_rate, \"num_epoch=\", num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "begin training using triangular learning rate policy with max lr of 0.0001...\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      " 61662/119678 [==============>...............] - ETA: 34:05 - loss: 0.0905 - accuracy: 0.9731"
     ]
    }
   ],
   "source": [
    "# Many fit options: https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c\n",
    "# 1cycle learning rate policy that linearly increases the learning rate for the first half of training \n",
    "# and then decreases the learning rate for the latter half:\n",
    "\n",
    "learner.autofit(learning_rate, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC, AUC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_performance_aggregation(y_test, y_score, n_classes):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    return roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_performance_aggregation(fpr, tpr, n_classes):\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area    \n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    return roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_performance(fpr, tpr, roc_auc, n_classes):\n",
    "    \n",
    "    plt.figure()\n",
    "    line_width = 2\n",
    "    \n",
    "#     plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "#              label='micro-average ROC curve (area = {0:0.2f})'\n",
    "#                    ''.format(roc_auc[\"micro\"]),\n",
    "#              color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "#     plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "#              label='macro-average ROC curve (area = {0:0.2f})'\n",
    "#                    ''.format(roc_auc[\"macro\"]),\n",
    "#              color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=line_width,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=line_width)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = learner.model.predict(x_test, verbose=0)\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Receiver Operating Characteristic: Area Under the Curve score: %.6f\" % (auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc, fpr, tpr = class_performance_aggregation(y_test, y_pred, n_classes)\n",
    "\n",
    "roc_auc, fpr, tpr = multiclass_performance_aggregation(fpr, tpr, n_classes)\n",
    "\n",
    "plot_multiclass_performance(fpr, tpr, roc_auc, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ktrain.get_predictor(learner.model, pre_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = {}\n",
    "pred_dictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_set == 'jigsaw':\n",
    "    input_texts[0] = [\"If you don't stop immediately, I will kill you.\"]\n",
    "    input_texts[1] = [\"Okay - I'm calling it a night. See you tomorrow.\"]\n",
    "    input_texts[2] = [\"You have a really ugly face.\"]\n",
    "if data_set == 'casetext':\n",
    "    input_texts[0] = [\"Widomski failed to demonstrate that OCCC's explanation for its decision to bring disciplinary proceedings against him was pretext for retaliation. “Claims for retaliation [under the ADA] are analyzed under the same burden-shifting framework established for Title VII cases.” Treglia v. Town of Manlius, 313 F.3d 713, 719 (2d Cir.2002). For substantially the reasons set forth in the District Court's Opinion and Order entered March 21, 2013, we agree that Widomski has not produced evidence that would permit a reasonable factfinder to conclude that the initiation of disciplinary proceedings against him was a pretext for retaliation. Contarino's good faith belief that Widomski fabricated two assignments constitutes a legitimate, nonretaliatory reason for bringing a disciplinary action against him. The burden having shifted back to Widomski to provide competent evidence of pretext, Widomski fails to raise a genuine factual dispute as to whether this explanation is false or otherwise pretextual. See Weinstock v. Columbia Univ., 224 F.3d 33, 42 (2d Cir.2000). CONCLUSION We have considered Widomski's remaining arguments and conclude that they are without merit. For the foregoing reasons, the judgment of the District Court is AFFIRMED.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in input_texts.items():\n",
    "    prediction = predictor.predict(v)\n",
    "    print(\"prediction=\", prediction)\n",
    "    pred_dictions[k] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(input_texts)):  \n",
    "    print(i, \"input_texts=\", input_texts[i])\n",
    "    print(i, \"predictions=\", pred_dictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SAVE, LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.save(NLP_ROOT + 'multilabel_detector')\n",
    "predictor = ktrain.load_predictor(NLP_ROOT + 'multilabel_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(input_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blog: IMDB https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358\n",
    "# Demo: IMDB https://colab.research.google.com/drive/18SVeIFXWCiA9HL4WVCAFxlfH59ez6atc#scrollTo=p5lJ5jXpqtM9\n",
    "# Demo: Newsgroup https://colab.research.google.com/drive/1AH3fkKiEqBpVpO5ua00scp7zcHs5IDLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
